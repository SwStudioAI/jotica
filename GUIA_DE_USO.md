# üôè Jotica Bible - Gu√≠a de Uso Completa

## üöÄ Inicio R√°pido

### Para Probar Jotica Ahora Mismo (SimplePod)
```bash
# Conectar a tu servidor entrenado
ssh -i "~/.ssh/simplepod_key" root@176.9.144.36

# Ir al proyecto y activar entorno
cd ~/jotica-bible && source venv/bin/activate

# Conversaci√≥n simple con Jotica
python3 -c "
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer, GenerationConfig
import torch

# Cargar modelo entrenado con personalidad
print('ü§ñ Cargando Jotica...')
model = AutoPeftModelForCausalLM.from_pretrained(
    'models/jotica-personality',
    torch_dtype=torch.float16,
    device_map='auto'
)
tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')
tokenizer.pad_token = tokenizer.eos_token

print('‚úÖ ¬°Jotica lista!')
print('üí¨ Preg√∫ntale algo b√≠blico...')
print()

# Configurar respuestas naturales
generation_config = GenerationConfig(
    max_length=200,
    do_sample=True, 
    temperature=0.8,
    top_p=0.9,
    repetition_penalty=1.1,
    pad_token_id=tokenizer.eos_token_id
)

# Preguntas de ejemplo
preguntas = [
    '¬øC√≥mo puedo perdonar a alguien que me lastim√≥?',
    '¬øQu√© dice la Biblia sobre la ansiedad?',
    '¬øC√≥mo puedo acercarme m√°s a Dios?'
]

for pregunta in preguntas:
    print(f'üë§ T√∫: {pregunta}')
    
    # Formatear como conversaci√≥n natural
    prompt = f'Usuario: {pregunta}\nJotica:'
    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)
    
    # Generar respuesta
    with torch.no_grad():
        outputs = model.generate(**inputs, generation_config=generation_config)
    
    # Extraer respuesta
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if 'Jotica:' in response:
        response = response.split('Jotica:')[-1].strip()
    
    print(f'üôè Jotica: {response}')
    print('-' * 50)
    print()
"
```

## ü§ñ Ejemplos de Conversaci√≥n

### Pregunta T√≠pica
```
üë§ Usuario: "Me siento muy ansioso por el futuro, ¬øqu√© puedo hacer?"

üôè Jotica: "Entiendo perfectamente esa preocupaci√≥n por el futuro, es algo muy humano. La Biblia nos recuerda en Filipenses 4:6-7 que no nos afanemos por nada, sino que en todo, por oraci√≥n y ruego, con acci√≥n de gracias, presentemos nuestras peticiones a Dios. 

Cuando Jes√∫s ense√±√≥ en Mateo 6:26, nos dijo que mir√°ramos las aves del cielo: no siembran, ni siegan, pero nuestro Padre celestial las alimenta. ¬øNo valemos nosotros mucho m√°s que ellas?

Te animo a que cada ma√±ana le entregues tus preocupaciones a Dios en oraci√≥n. √âl conoce tus necesidades antes de que se las pidas. ¬øHas probado escribir tus inquietudes y despu√©s orar espec√≠ficamente por cada una?"
```

### Respuesta sobre Perd√≥n
```
üë§ Usuario: "Alguien me traicion√≥ y no s√© si puedo perdonarlo"

üôè Jotica: "Ay, qu√© dolor tan profundo debe ser eso. La traici√≥n de alguien en quien confi√°bamos es una de las heridas m√°s dif√≠ciles de sanar. Te comprendo completamente.

Jes√∫s nos ense√±√≥ en Mateo 18:21-22 que debemos perdonar no solo siete veces, sino setenta veces siete. Pero esto no significa que debemos olvidar o confiar ciegamente otra vez.

El perd√≥n es m√°s para tu propia libertad que para la otra persona. Como dice Efesios 4:32, perdonamos porque Cristo nos perdon√≥ primero. Es un proceso, no un evento de una sola vez.

¬øTe ayudar√≠a comenzar orando por esa persona? No necesariamente pidiendo bendiciones, sino simplemente llev√°ndola ante Dios. A veces el perd√≥n comienza ah√≠, en la oraci√≥n."
```

## üõ†Ô∏è Uso T√©cnico Avanzado

### Cargar Modelo Program√°ticamente
```python
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer, GenerationConfig
import torch

class JoticaBible:
    def __init__(self, model_path="models/jotica-personality"):
        """Inicializar Jotica con modelo entrenado"""
        self.model = AutoPeftModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        self.tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-small")
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.generation_config = GenerationConfig(
            max_length=300,
            do_sample=True,
            temperature=0.8,
            top_p=0.9,
            top_k=50,
            repetition_penalty=1.15,
            pad_token_id=self.tokenizer.eos_token_id,
            eos_token_id=self.tokenizer.eos_token_id
        )
    
    def responder(self, pregunta):
        """Generar respuesta b√≠blica emp√°tica"""
        prompt = f"Usuario: {pregunta}\nJotica:"
        
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                generation_config=self.generation_config
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extraer solo la respuesta de Jotica
        if "Jotica:" in response:
            response = response.split("Jotica:")[-1].strip()
        
        return response
    
    def conversacion_interactiva(self):
        """Modo conversaci√≥n interactiva"""
        print("üôè ¬°Hola! Soy Jotica, tu asistente b√≠blico.")
        print("üí¨ Puedes preguntarme sobre la Biblia, fe, o cualquier inquietud espiritual.")
        print("‚ú® Escribe 'salir' para terminar la conversaci√≥n.\n")
        
        while True:
            pregunta = input("üë§ T√∫: ").strip()
            
            if pregunta.lower() in ['salir', 'exit', 'quit', 'adi√≥s']:
                print("üôè ¬°Que Dios te bendiga! Espero haberte ayudado.")
                break
            
            if not pregunta:
                continue
            
            print("ü§î Jotica est√° pensando...")
            respuesta = self.responder(pregunta)
            print(f"üôè Jotica: {respuesta}\n")

# Uso
if __name__ == "__main__":
    jotica = JoticaBible()
    jotica.conversacion_interactiva()
```

### API REST con FastAPI
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn

app = FastAPI(title="Jotica Bible API", version="1.0.0")
jotica = None

class PreguntaRequest(BaseModel):
    pregunta: str
    temperatura: float = 0.8

class RespuestaResponse(BaseModel):
    pregunta: str
    respuesta: str
    version_modelo: str

@app.on_event("startup")
async def cargar_modelo():
    global jotica
    jotica = JoticaBible("models/jotica-personality")
    print("‚úÖ Jotica cargada y lista")

@app.post("/preguntar", response_model=RespuestaResponse)
async def hacer_pregunta(request: PreguntaRequest):
    if not jotica:
        raise HTTPException(status_code=503, detail="Modelo no cargado")
    
    try:
        respuesta = jotica.responder(request.pregunta)
        return RespuestaResponse(
            pregunta=request.pregunta,
            respuesta=respuesta,
            version_modelo="jotica-personality-v1"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

@app.get("/salud")
async def check_salud():
    return {"estado": "saludable", "modelo": "jotica-personality-v1"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## üîß Configuraciones Avanzadas

### Personalizar Respuestas
```python
# Respuestas m√°s cortas y directas
generation_config = GenerationConfig(
    max_length=150,
    temperature=0.6,
    top_p=0.8,
    repetition_penalty=1.1
)

# Respuestas m√°s creativas y largas  
generation_config = GenerationConfig(
    max_length=400,
    temperature=1.0,
    top_p=0.95,
    top_k=40,
    repetition_penalty=1.2
)

# Respuestas muy conservadoras
generation_config = GenerationConfig(
    max_length=200,
    do_sample=False,  # Determin√≠stico
    num_beams=5,
    early_stopping=True
)
```

### Filtros de Contenido
```python
def validar_pregunta(pregunta):
    """Validar que la pregunta sea apropiada"""
    palabras_prohibidas = ["violencia", "odio", "discriminaci√≥n"]
    
    pregunta_lower = pregunta.lower()
    for palabra in palabras_prohibidas:
        if palabra in pregunta_lower:
            return False, f"No puedo responder preguntas sobre {palabra}"
    
    return True, "Pregunta v√°lida"

def mejorar_respuesta(respuesta):
    """Mejorar formato de respuesta"""
    # Agregar vers√≠culos si no los tiene
    if "Biblia" in respuesta and not any(libro in respuesta for libro in ["Mateo", "Juan", "G√©nesis", "Salmos"]):
        respuesta += "\n\nüí° Te recomiendo leer m√°s en los Salmos para profundizar en este tema."
    
    return respuesta
```

## üìä Monitoreo y Logs

### Logging Detallado
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('jotica.log'),
        logging.StreamHandler()
    ]
)

def responder_con_logs(self, pregunta):
    """Responder con logging detallado"""
    logging.info(f"Nueva pregunta recibida: {pregunta[:50]}...")
    
    start_time = time.time()
    respuesta = self.responder(pregunta)
    end_time = time.time()
    
    logging.info(f"Respuesta generada en {end_time - start_time:.2f}s")
    logging.info(f"Longitud respuesta: {len(respuesta)} caracteres")
    
    return respuesta
```

## üåê Despliegue en Producci√≥n

### Docker para Render.com
```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["python", "api_server.py"]
```

### Variables de Entorno
```bash
# Render.com Environment Variables
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-anon-key
MODELO_PATH=models/jotica-personality
PORT=8000
```

## üéØ Casos de Uso Recomendados

### 1. Consejer√≠a B√≠blica Personal
- Preguntas sobre relaciones, ansiedad, prop√≥sito
- Respuestas emp√°ticas con base escritural
- Seguimiento conversacional natural

### 2. Estudio B√≠blico Interactivo  
- Explicaci√≥n de pasajes
- Aplicaci√≥n pr√°ctica de principios
- Contexto hist√≥rico y cultural

### 3. Oraci√≥n y Meditaci√≥n
- Gu√≠as de oraci√≥n personalizadas
- Reflexiones diarias
- Vers√≠culos para memorizar

### 4. Apoyo Emocional Cristiano
- Comfort en tiempos dif√≠ciles
- Esperanza basada en las Escrituras
- Perspectiva b√≠blica sobre problemas actuales

## üìû Soporte y Resoluci√≥n de Problemas

### Problemas Comunes

**Error de memoria CUDA:**
```bash
export CUDA_VISIBLE_DEVICES=0
python3 -c "import torch; torch.cuda.empty_cache()"
```

**Modelo no carga:**
```python
# Verificar archivos
import os
print(os.listdir("models/jotica-personality"))

# Cargar sin GPU
model = AutoPeftModelForCausalLM.from_pretrained(
    "models/jotica-personality",
    device_map="cpu",
    torch_dtype=torch.float32
)
```

**Respuestas repetitivas:**
```python
# Aumentar repetition_penalty
generation_config.repetition_penalty = 1.3

# Usar nucleus sampling
generation_config.top_p = 0.8
generation_config.temperature = 0.9
```

---

## üôè Bendiciones y Prop√≥sito

Jotica ha sido creada para ser un puente entre la sabidur√≠a eterna de las Escrituras y las necesidades cotidianas de las personas. Su prop√≥sito es ofrecer consuelo, gu√≠a y esperanza basados en la Palabra de Dios, siempre con amor, comprensi√≥n y respeto.

*"La hierba se seca, la flor se marchita; mas la palabra del Dios nuestro permanece para siempre." - Isa√≠as 40:8*

**¬°Que Jotica sea de bendici√≥n para muchas vidas!** üôè‚ú®