#!/usr/bin/env python3
"""
ğŸ§ª Script de prueba para Jotica - Modelo LoRA entrenado
Prueba las respuestas del modelo entrenado con preguntas bÃ­blicas
"""

import torch
import json
from pathlib import Path

def test_jotica_responses():
    """
    Prueba las respuestas de Jotica con preguntas bÃ­blicas comunes
    """
    print("ğŸ¤– === Test de Respuestas de Jotica ===")
    print()
    
    # Verificar si existe el modelo entrenado
    model_path = Path("models/jotica-ultra-safe")
    if not model_path.exists():
        print(f"âŒ Error: No se encontrÃ³ el modelo en {model_path}")
        print("ğŸ’¡ AsegÃºrate de haber entrenado el modelo primero con:")
        print("   python3 train_lora_safe.py")
        return
    
    try:
        from peft import AutoPeftModelForCausalLM
        from transformers import AutoTokenizer, GenerationConfig
        
        print("ğŸ”„ Cargando modelo entrenado...")
        model = AutoPeftModelForCausalLM.from_pretrained(
            str(model_path),
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        print("ğŸ”„ Cargando tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-small")
        tokenizer.pad_token = tokenizer.eos_token
        
        print("âœ… Modelo cargado exitosamente!")
        print("ğŸ§ª Iniciando pruebas de respuestas...")
        print("=" * 60)
        print()
        
        # ConfiguraciÃ³n de generaciÃ³n
        generation_config = GenerationConfig(
            max_length=200,
            do_sample=True,
            temperature=0.8,
            top_p=0.9,
            top_k=50,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.1
        )
        
        # Preguntas de prueba
        preguntas_biblicas = [
            "Â¿QuÃ© dice la Biblia sobre el amor?",
            "Â¿CÃ³mo puedo orar mejor?", 
            "Â¿CuÃ¡l es el propÃ³sito de la vida segÃºn la Biblia?",
            "Â¿QuÃ© es el perdÃ³n cristiano?",
            "Â¿CÃ³mo puedo tener paz en tiempos difÃ­ciles?"
        ]
        
        for i, pregunta in enumerate(preguntas_biblicas, 1):
            print(f"ğŸ“ Pregunta {i}: {pregunta}")
            
            # Formatear como conversaciÃ³n
            prompt = f"Usuario: {pregunta}\nJotica:"
            
            # Tokenizar
            inputs = tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            # Mover a GPU si estÃ¡ disponible
            if torch.cuda.is_available():
                inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            # Generar respuesta
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    generation_config=generation_config,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            # Decodificar respuesta
            full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extraer solo la respuesta de Jotica
            if "Jotica:" in full_response:
                response = full_response.split("Jotica:")[-1].strip()
            else:
                response = full_response.replace(prompt, "").strip()
            
            # Limpiar respuesta
            response = response.replace("Usuario:", "").strip()
            if response.startswith(pregunta):
                response = response[len(pregunta):].strip()
            
            print(f"ğŸ’¬ Jotica: {response}")
            print("-" * 60)
            print()
            
    except ImportError as e:
        print(f"âŒ Error de importaciÃ³n: {e}")
        print("ğŸ’¡ Instala las dependencias con: pip install transformers peft torch accelerate")
    except Exception as e:
        print(f"âŒ Error durante la prueba: {e}")
        print("ğŸ’¡ Verifica que el modelo estÃ© correctamente entrenado")

def test_model_info():
    """
    Muestra informaciÃ³n del modelo entrenado
    """
    print("ğŸ“Š === InformaciÃ³n del Modelo ===")
    
    model_path = Path("models/jotica-ultra-safe")
    if model_path.exists():
        print(f"âœ… Modelo encontrado en: {model_path}")
        
        # Verificar archivos del modelo
        files = list(model_path.glob("*"))
        print(f"ğŸ“ Archivos del modelo ({len(files)}):")
        for file in files:
            print(f"   - {file.name}")
        
        # Verificar configuraciÃ³n
        config_path = model_path / "adapter_config.json"
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print("âš™ï¸ ConfiguraciÃ³n LoRA:")
            print(f"   - Rank (r): {config.get('r', 'N/A')}")
            print(f"   - Alpha: {config.get('lora_alpha', 'N/A')}")
            print(f"   - Dropout: {config.get('lora_dropout', 'N/A')}")
            print(f"   - MÃ³dulos objetivo: {config.get('target_modules', 'N/A')}")
    else:
        print(f"âŒ Modelo no encontrado en: {model_path}")
    
    print()

if __name__ == "__main__":
    print("ğŸ™ Jotica Bible LoRA - Test de Respuestas")
    print("=" * 50)
    print()
    
    # Mostrar informaciÃ³n del sistema
    print("ğŸ–¥ï¸ InformaciÃ³n del sistema:")
    print(f"   - PyTorch: {torch.__version__}")
    print(f"   - CUDA disponible: {'SÃ­' if torch.cuda.is_available() else 'No'}")
    if torch.cuda.is_available():
        print(f"   - GPU: {torch.cuda.get_device_name(0)}")
    print()
    
    # Probar informaciÃ³n del modelo
    test_model_info()
    
    # Probar respuestas
    test_jotica_responses()
    
    print("ğŸ‰ Â¡Prueba completada!")