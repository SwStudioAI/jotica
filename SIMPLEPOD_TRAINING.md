# üöÄ Gu√≠a Completa: Entrenar LoRA B√≠blico en SimplePod.ai

## üìã Prerrequisitos

1. **Cuenta en SimplePod.ai**: Reg√≠strate en https://simplepod.ai/
2. **Cr√©ditos GPU**: Aseg√∫rate de tener cr√©ditos para GPU (recomendado: A100 40GB)
3. **Archivos del repositorio**: Todos los archivos de `jotica-bible` listos

---

## üèÉ‚Äç‚ôÇÔ∏è Pasos para Entrenar

### 1Ô∏è‚É£ **Crear Nuevo Pod en SimplePod**

1. **Navega a SimplePod.ai**:
   - Ve a https://simplepod.ai/dashboard
   - Haz clic en "New Pod"

2. **Configuraci√≥n del Pod**:
   ```
   Pod Name: jotica-bible-training
   Image: pytorch/pytorch:2.1.0-cuda12.1-cudnn8-devel
   GPU: A100 40GB (recomendado) o RTX 4090
   Storage: 50GB m√≠nimo
   ```

3. **Variables de Entorno**:
   ```bash
   SUPABASE_URL=https://jmtrhukymrhzrmqmgrfq.supabase.co
   SUPABASE_SERVICE_ROLE=tu_service_role_key_aqui
   HUGGING_FACE_HUB_TOKEN=tu_hf_token_aqui  # opcional
   ```

### 2Ô∏è‚É£ **Subir Archivos al Pod**

**Opci√≥n A: Clonar desde GitHub**
```bash
git clone https://github.com/tu-usuario/jotica-bible.git
cd jotica-bible
```

**Opci√≥n B: Subir archivos manualmente**
- Usa la interfaz de SimplePod para subir el zip del repositorio
- Extrae en `/workspace/jotica-bible/`

### 3Ô∏è‚É£ **Configuraci√≥n Inicial**

1. **Conecta al Pod**:
   - Haz clic en "Connect" en tu pod
   - Abre terminal o Jupyter

2. **Ejecuta el bootstrap**:
   ```bash
   cd /workspace/jotica-bible
   chmod +x scripts/*.sh
   ./scripts/bootstrap_simplepod.sh
   ```

   Esto instalar√°:
   - ‚úÖ Dependencias Python
   - ‚úÖ Datos b√≠blicos procesados
   - ‚úÖ Configuraci√≥n de entorno

### 4Ô∏è‚É£ **Preparar Datos B√≠blicos**

```bash
# Procesar la Biblia RVA 1909
./scripts/ingest_bible.sh

# Verificar datos procesados
ls -la data/processed/
```

Deber√≠as ver:
```
bible_qa_alpaca.json    # Dataset en formato Alpaca
bible_embeddings.npz    # Embeddings vectoriales  
bible_metadata.json    # Metadatos y estad√≠sticas
```

### 5Ô∏è‚É£ **Iniciar Entrenamiento LoRA**

**Entrenamiento R√°pido (Prueba)**:
```bash
./scripts/train_lora.sh --quick
```

**Entrenamiento Completo**:
```bash
./scripts/train_lora.sh \
  --model meta-llama/Llama-3-8B-Instruct \
  --epochs 3 \
  --batch-size 4 \
  --lr 2e-4 \
  --run-name "jotica-v1"
```

**Entrenamiento Personalizado**:
```bash
python src/train/lora_runner.py \
  --base-model meta-llama/Llama-3-8B-Instruct \
  --data data/processed/bible_qa_alpaca.json \
  --output models/jotica-lora \
  --run-name jotica-experimental \
  --epochs 5 \
  --batch-size 8 \
  --lr 1e-4 \
  --max-seq-len 2048
```

---

## üìä Monitoreo del Entrenamiento

### **Logs en Tiempo Real**
```bash
# Ver progreso
tail -f logs/training.log

# Monitorear GPU
nvidia-smi -l 1
```

### **M√©tricas Esperadas**
```
üöÄ Starting LoRA training: jotica-v1
   Model: meta-llama/Llama-3-8B-Instruct
   Dataset: 15,847 samples
   Epochs: 3
   Batch size: 4
   Output: models/jotica-lora

Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3962/3962 [45:32<00:00, 1.45it/s]
Training Loss: 1.234

üì¶ Creating archive: jotica_jotica-v1_20241220_143022.tar.gz
‚òÅÔ∏è Uploading to Supabase Storage...
üéâ Checkpoint successfully backed up to Supabase!
```

### **Tiempos Estimados** (A100 40GB)
- Dataset completo (15K samples): ~2-3 horas
- Prueba r√°pida (1K samples): ~15-20 minutos
- Upload a Supabase: ~2-5 minutos

---

## ‚òÅÔ∏è Respaldo Autom√°tico

El sistema **guarda autom√°ticamente** en Supabase:

### **¬øQu√© se guarda?**
- ‚úÖ Modelo LoRA entrenado (`adapter_model.bin`)
- ‚úÖ Configuraci√≥n (`adapter_config.json`) 
- ‚úÖ Tokenizer completo
- ‚úÖ Metadatos de entrenamiento (`training_info.json`)
- ‚úÖ Logs de entrenamiento

### **¬øD√≥nde se guarda?**
```
Supabase Storage > jotica-models bucket:
‚îú‚îÄ‚îÄ jotica-v1/
‚îÇ   ‚îî‚îÄ‚îÄ jotica_jotica-v1_20241220_143022.tar.gz
‚îú‚îÄ‚îÄ jotica-experimental/  
‚îÇ   ‚îî‚îÄ‚îÄ jotica_jotica-experimental_20241220_154511.tar.gz
‚îî‚îÄ‚îÄ ...
```

### **Verificar Backup**
```bash
# Listar checkpoints en Supabase
python -c "from src.utils.supa import list_checkpoints; print(list_checkpoints())"
```

---

## üîß Soluci√≥n de Problemas

### **Error: CUDA Out of Memory**
```bash
# Reducir batch size
python src/train/lora_runner.py --batch-size 2

# O usar gradient accumulation  
python src/train/lora_runner.py --batch-size 1 --gradient-accumulation-steps 4
```

### **Error: Supabase Connection**
```bash
# Verificar variables de entorno
echo $SUPABASE_URL
echo $SUPABASE_SERVICE_ROLE

# Probar conexi√≥n
python -c "from src.utils.supa import test_connection; test_connection()"
```

### **Error: Modelo No Encontrado**
```bash
# Verificar acceso HuggingFace
huggingface-cli login

# O usar modelo local
python src/train/lora_runner.py --base-model ./models/llama-3-8b-local/
```

---

## üöÄ Siguientes Pasos

### **1. Probar el Modelo Entrenado**
```bash
# Descargar desde Supabase
python src/utils/supa.py download jotica_jotica-v1_20241220_143022.tar.gz

# Extraer e iniciar API
tar -xzf jotica_*.tar.gz
python src/api/server.py --model-path ./models/jotica-lora/
```

### **2. Hacer Inferencia**
```bash
curl -X POST "http://localhost:8000/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "¬øQu√© dice la Biblia sobre el amor?",
    "max_length": 200
  }'
```

### **3. Entrenar Nuevas Versiones**
- Ajusta hiperpar√°metros
- Usa diferentes conjuntos de datos  
- Experimenta con otros modelos base

---

## üí° Tips de Optimizaci√≥n

### **Para Entrenamiento M√°s R√°pido**
- Usar A100 80GB si est√° disponible
- Aumentar `batch_size` a 8-16
- Usar `fp16=True` para ahorrar memoria

### **Para Mejor Calidad**
- Aumentar `epochs` a 5-10
- Usar `lr=1e-4` (learning rate m√°s bajo)
- Aumentar `max_seq_len` a 4096

### **Para Experimentar**
- Cambiar `base_model` a otros modelos
- Modificar datos en `data/processed/`
- Ajustar par√°metros LoRA en `src/config.py`

---

## üÜò Soporte

- **Logs**: Revisa `logs/training.log` 
- **GitHub Issues**: Abre issue en el repositorio
- **SimplePod Support**: Contacta support@simplepod.ai
- **Supabase Dashboard**: https://app.supabase.com/project/jmtrhukymrhzrmqmgrfq/

---

¬°**Feliz entrenamiento!** üéâ Tu LoRA b√≠blico estar√° listo en unas horas y respaldado autom√°ticamente en Supabase. ‚ú®