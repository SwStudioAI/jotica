<div align="center">

# üïäÔ∏è Jotica - Biblical LoRA Training Platform

*Advanced AI training system for biblical texts using LoRA fine-tuning*

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=flat&logo=docker&logoColor=white)](https://www.docker.com/)
[![GPU](https://img.shields.io/badge/CUDA-12.1+-green.svg?logo=nvidia)](https://developer.nvidia.com/cuda-toolkit)

[üöÄ Quick Start](#-quick-start-on-simplepod) ‚Ä¢ [üìñ Documentation](#-project-structure) ‚Ä¢ [‚ö° Features](#-key-features) ‚Ä¢ [ü§ù Contributing](#-contributing)

</div>

---

## üéØ Overview

**Jotica** is a comprehensive system for training **LoRA (Low-Rank Adaptation)** models on biblical texts, specifically designed for the **Reina-Valera 1909** Spanish Bible. Built for **SimplePod Docker environments** with **GPU acceleration**, it enables efficient fine-tuning of large language models for theological and biblical applications.

### üåü Key Features

- üî• **LoRA Fine-tuning** - Parameter-efficient training with PEFT
- üê≥ **SimplePod Ready** - Optimized for Docker + GPU environments  
- üìö **Biblical Focus** - Specialized for Reina-Valera 1909 texts
- üéØ **Alpaca Format** - Industry-standard instruction formatting
- ‚òÅÔ∏è **Supabase Integration** - Vector storage and model checkpoints
- ‚ö° **OpenAI Embeddings** - High-quality semantic representations
- üìä **Automated Pipeline** - From ingestion to deployment

## üèóÔ∏è Project Structure

```
jotica/
‚îú‚îÄ‚îÄ üîß config/
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt      # Python dependencies
‚îÇ   ‚îú‚îÄ‚îÄ .env.example         # Environment template
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile           # SimplePod container
‚îú‚îÄ‚îÄ üöÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap_simplepod.sh   # One-time setup
‚îÇ   ‚îú‚îÄ‚îÄ ingest_bible.sh         # Data processing
‚îÇ   ‚îî‚îÄ‚îÄ train_lora.sh           # Training pipeline
‚îú‚îÄ‚îÄ üìÇ data/
‚îÇ   ‚îú‚îÄ‚îÄ bible_rva1909/          # Raw biblical texts
‚îÇ   ‚îî‚îÄ‚îÄ refs/                   # Reference materials
‚îú‚îÄ‚îÄ üéì datasets/
‚îÇ   ‚îî‚îÄ‚îÄ bible_qa.jsonl          # Curated Q&A training data
‚îî‚îÄ‚îÄ üíª src/
    ‚îú‚îÄ‚îÄ config.py               # Configuration management
    ‚îú‚îÄ‚îÄ utils/                  # Core utilities
    ‚îÇ   ‚îú‚îÄ‚îÄ supa.py            # Supabase client
    ‚îÇ   ‚îú‚îÄ‚îÄ emb.py             # Embedding generation
    ‚îÇ   ‚îî‚îÄ‚îÄ io_utils.py        # File I/O helpers
    ‚îú‚îÄ‚îÄ ingest/                 # Data processing
    ‚îÇ   ‚îú‚îÄ‚îÄ parse_bible.py     # Bible text parser
    ‚îÇ   ‚îú‚îÄ‚îÄ parse_refs.py      # Reference processing
    ‚îÇ   ‚îî‚îÄ‚îÄ upsert_supabase.py # Vector database
    ‚îú‚îÄ‚îÄ train/                  # Training modules
    ‚îÇ   ‚îú‚îÄ‚îÄ formatters.py      # Alpaca formatting
    ‚îÇ   ‚îú‚îÄ‚îÄ lora_runner.py     # LoRA training engine
    ‚îÇ   ‚îî‚îÄ‚îÄ eval_smoke.py      # Model evaluation
    ‚îî‚îÄ‚îÄ inference/              # Model serving
        ‚îî‚îÄ‚îÄ generate.py        # Text generation
```

## üöÄ Quick Start on SimplePod

### Prerequisites
- üê≥ **SimplePod** environment with GPU support
- üîë **API Keys**: OpenAI, Supabase
- üíæ **Storage**: ~10GB for models and data

### 1Ô∏è‚É£ Clone & Setup
```bash
# Clone the repository
git clone https://github.com/SwStudioAI/jotica.git
cd jotica

# Configure environment
cp .env.example .env
# ‚úèÔ∏è Edit .env with your API keys
```

### 2Ô∏è‚É£ Bootstrap Environment
```bash
# One-time setup in SimplePod
bash scripts/bootstrap_simplepod.sh
```

### 3Ô∏è‚É£ Data Preparation
```bash
# Add your biblical texts to data/bible_rva1909/
# Add reference materials to data/refs/

# Process and create embeddings
bash scripts/ingest_bible.sh
```

### 4Ô∏è‚É£ Train LoRA Model
```bash
# Start training pipeline
bash scripts/train_lora.sh
```

### 5Ô∏è‚É£ Test Your Model
```bash
# Interactive testing
python -m src.inference.generate "Explica Juan 1:1 con citas b√≠blicas"
```

## ‚öôÔ∏è Configuration

### Environment Variables (.env)
```bash
# üîë API Keys
OPENAI_API_KEY=your_openai_key_here
SUPABASE_URL=your_supabase_url
SUPABASE_SERVICE_ROLE=your_service_key

# ‚òÅÔ∏è Storage
SUPABASE_BUCKET=jotica-models        # Model checkpoints
SUPABASE_DATA_BUCKET=jotica-data     # Optional corpus storage

# ü§ñ Model Configuration  
BASE_MODEL=meta-llama/Llama-3-8B-Instruct
RUN_NAME=jotica-bible-lora-001
OUTPUT_DIR=/workspace/jotica/checkpoints

# üéØ Training Parameters
EPOCHS=2
BATCH_SIZE=2
LR=2e-4
SAVE_STEPS=200
SAVE_TOTAL=3
MAX_SEQ_LEN=1024
```

### LoRA Configuration
```python
LoraConfig(
    r=16,                    # Low-rank dimension
    lora_alpha=32,          # Scaling parameter
    lora_dropout=0.05,      # Dropout rate
    target_modules=["q_proj", "v_proj"]  # Target attention layers
)
```

## üìä Training Pipeline

```mermaid
graph LR
    A[üìö Raw Texts] --> B[üîÑ Parse Bible]
    B --> C[üß† Generate Embeddings]
    C --> D[‚òÅÔ∏è Supabase Storage]
    D --> E[üéì Format Training Data]
    E --> F[üî• LoRA Training]
    F --> G[üíæ Save Checkpoint]
    G --> H[‚òÅÔ∏è Upload to Storage]
```

## üéØ Model Architecture

- **Base Model**: Llama-3-8B-Instruct
- **Fine-tuning**: LoRA (Low-Rank Adaptation)
- **Format**: Alpaca instruction format
- **Context Length**: 1024 tokens
- **Precision**: FP16 for efficiency

## üìà Performance & Monitoring

- ‚úÖ **Automatic checkpointing** every 200 steps
- ‚úÖ **Supabase storage** backup
- ‚úÖ **Training logs** with loss tracking
- ‚úÖ **Smoke testing** for validation

## üê≥ Docker Deployment

```dockerfile
FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04
# Optimized for SimplePod GPU environments
```

Run with:
```bash
docker build -t jotica .
docker run --gpus all -v $(pwd):/workspace jotica
```

## ü§ù Contributing

1. Fork the repository on the `main` branch
2. Create your feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open a Pull Request to `main`

## üìã Data Format

### Training Data (bible_qa.jsonl)
```json
{
  "instruction": "Explica Juan 1:1 con enfoque en la deidad del Verbo",
  "input": "",
  "output": "Juan 1:1 (RVA1909): \"En el principio era el Verbo...\"",
  "topic": "cristolog√≠a"
}
```

### Biblical Verses
```json
{
  "book": "Juan",
  "chapter": 1,
  "verse": 1,
  "text": "En el principio era el Verbo, y el Verbo era con Dios..."
}
```

## üîß Troubleshooting

### Common Issues
- **GPU Memory**: Reduce `BATCH_SIZE` if OOM errors occur
- **API Limits**: Check OpenAI rate limits for embeddings
- **Supabase**: Verify connection and storage permissions

### Debug Mode
```bash
# Enable verbose logging
export DEBUG=1
bash scripts/train_lora.sh
```

## üìú License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- üìñ **Reina-Valera 1909** - Public domain biblical text
- ü§ó **Hugging Face** - Transformers and PEFT libraries
- ü¶ô **Meta AI** - Llama model architecture
- ‚òÅÔ∏è **Supabase** - Vector database and storage

---

<div align="center">

**Made with ‚ù§Ô∏è for Biblical AI Research**

[‚≠ê Star this repo](https://github.com/SwStudioAI/jotica) ‚Ä¢ [üêõ Report Bug](https://github.com/SwStudioAI/jotica/issues) ‚Ä¢ [üí° Request Feature](https://github.com/SwStudioAI/jotica/issues)

</div>
python src/ingest/create_embeddings.py
```

### 3. Generate Training Dataset

```bash
# Create Q&A pairs from biblical passages
python src/ingest/create_qa_dataset.py
```

### 4. Train LoRA Model

```bash
# Train on biblical Q&A dataset
python src/train/train_lora.py
```

### 5. Test Inference

```bash
# Test the trained model
python src/inference/test_model.py
```

## Docker Usage (SimplePod)

### Build Container

```bash
docker build -t jotica-bible .
```

### Run Training

```bash
docker run --gpus all \
  -v $(pwd)/models:/app/models \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/datasets:/app/datasets \
  -v $(pwd)/logs:/app/logs \
  --env-file .env \
  jotica-bible
```

### Interactive Development

```bash
docker run --gpus all -it \
  -v $(pwd):/app \
  --env-file .env \
  jotica-bible bash
```

## Configuration

### Environment Variables

Required environment variables (see `.env.example`):

- `OPENAI_API_KEY`: For generating embeddings and Q&A pairs
- `SUPABASE_URL`: Supabase project URL
- `SUPABASE_SERVICE_KEY`: Supabase service key
- `WANDB_API_KEY`: (Optional) For experiment tracking

### Training Configuration

Edit `src/train/config.py` to customize:
- Base model selection
- LoRA parameters (rank, alpha, dropout)
- Training hyperparameters
- Dataset configuration

## Features

### Data Processing
- **Biblical Text Parsing**: Extracts verses, chapters, and books from RVA 1909
- **Embedding Generation**: Creates vector embeddings using OpenAI's text-embedding-ada-002
- **Vector Storage**: Stores embeddings in Supabase for semantic search

### Q&A Dataset Creation
- **Automated Generation**: Creates question-answer pairs from biblical passages
- **Context Integration**: Includes surrounding verses for better context
- **Commentary Integration**: Incorporates biblical commentary and cross-references

### LoRA Training
- **Parameter Efficient**: Uses PEFT library for memory-efficient training
- **Multiple Base Models**: Supports various Spanish language models
- **Quantization Support**: 4-bit and 8-bit quantization for GPU efficiency

### Inference Pipeline
- **Semantic Search**: Find relevant biblical passages
- **Contextual Responses**: Generate responses based on biblical context
- **Multi-turn Conversations**: Support for extended biblical discussions

## Training Details

### Base Models Supported
- `microsoft/DialoGPT-medium` (Spanish fine-tuned)
- `PlanTL-GOB-ES/gpt2-base-bne`
- `huggingface/CodeBERTa-small-v1`

### LoRA Configuration
- Rank: 16-64 (configurable)
- Alpha: 32-128 (configurable)
- Target modules: All attention layers
- Dropout: 0.1

### Training Parameters
- Batch size: 4-8 (depending on GPU memory)
- Learning rate: 1e-4 to 5e-4
- Epochs: 3-10
- Gradient accumulation: 4-8 steps

## Monitoring and Logging

### Weights & Biases Integration
- Training metrics tracking
- Model performance monitoring
- Hyperparameter optimization

### Local Logging
- Training progress logs in `logs/`
- Model checkpoints in `models/`
- Dataset statistics and validation

## Data Sources

### Biblical Texts
- **Primary**: Reina-Valera 1909 Spanish Bible
- **Format**: JSON with book/chapter/verse structure
- **Processing**: Automatic verse parsing and validation

### Training Data Generation
- Context-aware Q&A pairs
- Cross-reference integration
- Commentary incorporation
- Thematic grouping

## GPU Requirements

### Recommended Setup
- **VRAM**: 16GB+ for full precision training
- **VRAM**: 8GB+ with quantization
- **Compute**: CUDA 11.8+ compatible

### SimplePod Compatibility
- Designed for SimplePod Docker GPU environments
- Automatic GPU detection and utilization
- Memory optimization for cloud instances

## Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/biblical-enhancement`)
3. Commit changes (`git commit -am 'Add biblical feature'`)
4. Push to branch (`git push origin feature/biblical-enhancement`)
5. Create Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- Reina-Valera 1909 Bible text
- Hugging Face Transformers library
- PEFT (Parameter Efficient Fine-Tuning)
- OpenAI API for embeddings
- Supabase for vector storage